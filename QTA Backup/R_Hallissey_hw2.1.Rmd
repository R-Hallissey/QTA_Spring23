---
title: 'POP77022: Programming Exercise 2'
author: "Ruairí Hallissey"
date: "3-3-2023 / 13 Ventôse CCXXXI"
output: html_document

Note: Apologies Martyn. I had many many issues and was running alorithms for over tweenty four hours in one case. I didn't atticpate how long it would take so this ultimately became extremely rushed and is only partcially complete.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The second homework assignment covers concepts and methods from Weeks 3 and 4 (Supervised and unsupervised text classification).  

Please provide your answers as code and text in the RMarkdown file provided. When completed, first knit the file as an HTML file and then save the resulting HTML document in PDF format.  Upload the PDF to Turnitin.

## Supervised text classification of Yelp reviews (50 points)

We begin by analyzing a sample from the Zhang, Zhao & LeCun (2015) dataset of Yelp reviews which have been coded for sentiment polarity.  The authors of the dataset have created a `sentiment` variable where a value of 1 indicates a "negative" review (1 or 2 stars), and a 2 means a "positive" review (3 or 4 stars).

First, bring in the reviews dataset from the `data` directory.  

```{r}
#Packages
pkgTest <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg,  dependencies = TRUE)
  sapply(pkg,  require,  character.only = TRUE)
}

lapply(c("tidyverse",
         "guardianapi",
         "quanteda", 
         "lubridate",
         "quanteda.textmodels", 
         "quanteda.textstats", 
         "caret", 
         "MLmetrics", 
         "doParallel",
         "naivebayes",
         "vctrs"),
       pkgTest)
```

1.  Create a `quanteda` corpus object from this matrix and inspect its attributes.  
    + What is the overall probability of the "positive" class in the corpus?  Are the classes balanced? (Hint: Use the `table()` function)

```{r}
### Problem 1 ###
# Data 
setwd(getwd())
yelp_data <- read.csv("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/ASDS/QTA/yelp_data_small.csv", 
                 stringsAsFactors=FALSE,
                 encoding = "utf-8")

# 1. Creating Corpus
yelp_corpus <- corpus(yelp_data,
                      meta = list(), # including meta data for names
                      unique_docnames = TRUE)
summary(yelp_corpus)

table(yelp_corpus$sentiment)
4912 / length(yelp_corpus$sentiment)
print("Probaility of a positive review is 0.49")
```

2.  Create a document-feature matrix using this corpus.  Process the text so as to increase predictive power of the features. Justify each of your processing decisions in the context of the supervised classification task.

```{r}
# 2. Processing Tokens and Creating Document Feature Matrix

# Fuctions
prep_toks <- function(text_corpus){
  toks <- tokens(text_corpus,
                 include_docvars = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english"), padding = TRUE) %>%
    tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE)
  return(toks)
}

get_coll <-function(tokens){
  unsup_col <- textstat_collocations(tokens,
                                     method = "lambda",
                                     size = 2,
                                     min_count = 5,
                                     smoothing = 0.5)
  
  unsup_col <- unsup_col[order(-unsup_col$count),] # sort detected collocations by count (descending)
  return(unsup_col)
}

prepped_toks <- prep_toks(yelp_corpus) # basic token cleaning
collocations <- get_coll(prepped_toks) # get collocations
toks <- tokens_compound(prepped_toks, pattern = collocations[collocations$z > 10,])
toks <- tokens_remove(tokens(toks), "") 

toks <- tokens(toks, 
               remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_symbols = TRUE,
               #remove_hyphens = TRUE,
               remove_separators = TRUE,
               remove_url = TRUE)

#lowercase
toks <- tokens_tolower(toks)

## Remove stop words
stop_list <- stopwords("english")
toks <- tokens_remove(toks, stop_list)

toks <- tokens_wordstem(toks)

# Create dfm
yelp_dfm <- dfm(toks) # create DFM
yelp_dfm <- dfm_trim(yelp_dfm, min_docfreq = 40) # drastically increased frequency cause when i ran at it took over 24hrs
                                                 # and by the time i stopped it i didn't have much time left
yelp_dfm <- dfm_tfidf(yelp_dfm)

# Create data frame
yelp_df <- convert(yelp_dfm, to = "data.frame", docvars = NULL)
yelp_df <- yelp_df[, -1] # drop document id variable (first variable)
sentiment_labels <- yelp_dfm@docvars$sentiment # get sentiment labels
yelp_df <- as.data.frame(cbind(sentiment_labels, yelp_df)) # labelled data frame
```

3.  Now that you have your document-feature matrix, use the `caret` library to create a training set and testing set following an 80/20 split.

```{r}
## ML Preparation
# Create a 5% validation split
set.seed(2023) # set seed for replicability
yelp_df <- yelp_df[sample(nrow(yelp_df)), ] # randomly order labelled dataset
split <- round(nrow(yelp_df) * 0.05) # determine cutoff point of 5% of documents
vdata <- yelp_df[1:split, ] # validation set
ldata <- yelp_df[(split + 1):nrow(yelp_df), ] #labelled dataset minus validation set

summary(ldata)

#b) Create an 80/20 test/train split
train_row_nums <- createDataPartition(ldata$sentiment_labels, 
                                      p=0.8, 
                                      list= FALSE) # set human_labels as the Y variable in caret
Train <- ldata[train_row_nums, ] # training set
Test <- ldata[-train_row_nums, ] # testing set

#             c) Create five-fold cross validation with 3 repeats object - to supply to train()
train_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  classProbs= TRUE, 
  summaryFunction = multiClassSummary,
  selectionFunction = "best", # select the model with the best performance metric
  verboseIter = TRUE
)
```

4.  Using these datasets, train a naive Bayes classifier with the `caret` library to predict review sentiment.  Explain each step you take in the learning pipeline. Be sure to:
    + Evaluate the performance of the model in terms of classification accuracy of predictions in the testing set. Include a discussion of precision, recall and F1.
    + Explain in detail what steps were taken to help avoid overfitting.
    + Describe your parameter tuning.
    + Discuss the most predictive features of the dataset. (*Hint: use `kwic` to provide a qualitative context)

```{r}
# Tuning Grid
tuneGrid = expand.grid(laplace = c(0,0.5,1.0),
                       usekernel = c(TRUE, FALSE),
                       adjust=c(0.75, 1, 1.25, 1.5))

# train model
#nb_train <- train(sentiment_labels ~ ., 
#                  data = Train,  
#                  method = "naive_bayes", 
#                  metric = "F1",
#                  trControl = train_control,
#                  tuneGrid = expand.grid(laplace = c(0,1),
#                                         usekernel = c(TRUE, FALSE),
#                                         adjust = c(0.75, 1, 1.25, 1.5)),
#                  allowParallel= TRUE
#)

#Training Model Upload
nb_train <- readRDS("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/ASDS/QTA/nb_train")

# Evaluate performance 
pred <- predict(nb_train, newdata = Test)
head(pred)


# confusion matrix
confusionMatrix(reference = as.factor(Test$sentiment_labels), data = pred, mode='everything', positive='neg')

# Final model code
#nb_final <- train(sentiment_labels ~ ., 
#                  data = ldata,  
#                  method = "naive_bayes", 
#                  trControl = trainControl(method = "none"),
#                  tuneGrid = data.frame(nb_train$bestTune))

# Final Model Upload
nb_final <- readRDS("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/ASDS/QTA/nb_final")

# Predict from validation set
pred2 <- predict(nb_final, newdata = vdata)
head(pred2) # first few predictions

# Evaluate confusion matrix
confusionMatrix(reference = as.factor(vdata$sentiment_labels), data = pred2, mode='everything')

#Analysis
print("In the testing set accuracy is 0.726 (95% CI :0.6846 - 0.7647), meaning 72% of the true negatives and true positives are classified accurately. Classification accuracy is significant (p <.001) meaning there is less .001% chance of obtaining these results by chance.

Precision, being true positives divided by false and negative positives is .82, meaning 82% of the positives are classified correctly. 

Recall, true positives divided by false negative and true positive, is 0.5743,meaning is 57.43% of data is correctly identified as belonging to the positive class. 

F1 is 0.67 meaning, an average 67% of all possible categories are correctly classified.
")

#Justification
print("Minimum frequency in the DFM was drastically increased to 40. >10 term frequncy was running for over 24hrs, after which I pulled the plug, so I needed something that run quickly.

Examining performance indicators of both models, they are very similar, suggesting model is not overfit to the training data.

5% cross validation split was also created to test out of sample. Split data gave predictive terms similar to training and main model such as hard, compar,  place, like, and  well

Used tuning grid divide the domain of the hyperparameters then calculated  performance metrics using cross-validation. ")

#validation
nb_final$coefnames

kwic(toks, phrase("hard"))
print("Hard often appears in context of people describing found too tough")

kwic(toks, phrase("place"))
print("Place often appears as people compliant the resteraunt")

kwic(toks, phrase("well"))
print("Well often appears as people compliant the resteraunt")
```

5. Provide a similar analysis using a Support Vector Machine.  However, irrespective of your settings for Question 4, for this excercise use a 5-fold cross-validation when training the model.  Be sure to explain all steps involved as well as an evaluation of model performance.  Which model is better, NB or SVM?  Explain in detail.

```{r}
#  grid
tuneGrid <- expand.grid(C = c(0.5, 1, 1.5))



#             d) Train the model
#svm_train <- train(sentiment_labels ~ ., 
 #                  data = Train,  
  #                 method = "svmLinear", 
   #                metric = "F1",
    #               trControl = train_control,
     #              tuneGrid = tuneGrid,
      #             allowParallel= TRUE
#)

svm_train <- readRDS("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/GitHub/QTA_Spring23/QTA Backup/svm_train") 


# Evaluate performance
print(svm_train)
pred_svm <- predict(svm_train, newdata = Test) # Predict on test sample using best model
confusionMatrix(reference = as.factor(Test$sentiment_labels), data = pred_svm, mode='everything')

#             i) Finalise by training on all labelled data
svm_final <- train(sentiment_labels ~ ., 
                   data = ldata,  
                   method = "svmLinear", 
                   trControl = trainControl(method = "none"),
                   tuneGrid = data.frame(svm_train$bestTune))
print(svm_final)
svm_final$coefnames


# read in the model
svm_final <- readRDS("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/ASDS/QTA")


#             l) Predict from validation set
svm_pred2 <- predict(svm_final, newdata = vdata)

#             m) Evaluate confusion matrix
confusionMatrix(reference = as.factor(vdata$sentiment_labels), data = svm_pred2, mode='everything')

# Analysis
print("In the testing set accuracy is0.834 (95% CI :0.7984 / 0.8656), meaning 82.31% of the true negatives and true positives are classified accurately. Classification accuracy is significant (p <.001) meaning there is less .001% chance of obtaining these results by chance.

Precision, being true positives divided by false and negative positives is 0.8487 , meaning 84.87% of the positives are classified correctly. 

Recall, true positives divided by false negative and true positive, is  0.8112 ,meaning is 81.12% of data is correctly identified as belonging to the positive class. 

F1 is  0.8296 meaning, an average 82.96% of all possible categories are correctly classified.

Super Vector Machine model has substnationally higher recall, and thus overall higher F1.
")

#Justication
print("Examining performance indicators of both training and test models, they are very similar, suggesting the model is not overfit to the training data.

SVM training and final models also gave predictive terms similar to thwe cross validation split.  

Used tuning grid divide the domain of the hyperparameters then calculated performance metrics using cross-validation") 


#Validation 
svm_final$coefnames

kwic(toks, phrase("hard"))
print("Hard often appears in context of people describing found too tough")

kwic(toks, phrase("place"))
print("Place often appears as people compliant the resteraunt")

kwic(toks, phrase("well"))
print("Well often appears as people compliant the resteraunt")
```

## Topic Modeling Breitbart News (50 points)

In this section, we will analyze the thematic structure of a corpus of news articles from Breitbart News, a right-wing American news outlet. Employ a Structural Topic Model from the `stm` library to investigate the themes found within this corpus.

First, bring in a sample of Breitbart articles from 2016 (n=5000):

```{r}
setwd(getwd())
dat <- read.csv("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/ASDS/QTA/breitbart_2016_sample.csv", 
                 stringsAsFactors=FALSE,
                 encoding = "utf-8")
```

1. Process the text and generate a document-feature matrix.  Be sure to remove unhelpful characters and tokens from the DFM and to also retain the original text for model validation.  Remove tokens that occur in less than 20 documents.  Justify your feature selection decisions.

```{r}
# Pre-Process data
dat$content <- str_replace(dat$content, "\u2022.+$", "")

# Creating corpus
corp <- corpus(dat, 
               docid_field = "title",
               text_field = "content")

# Processing
prepped_toks <- prep_toks(corp) # basic token cleaning
collocations <- get_coll(prepped_toks) # get collocations
toks <- tokens_compound(prepped_toks, pattern = collocations[collocations$z > 10,]) 

# replace collocations
toks <- tokens_remove(tokens(toks), "")  # let's also remove the whitespace placeholders
toks <- tokens_remove(toks, c("said","say"))

#Stopwords 
stop_list <- stopwords("english")
toks <- tokens_remove(toks, stop_list)


toks <- tokens(toks, 
               remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_symbols = TRUE,
               #remove_hyphens = TRUE,
               remove_separators = TRUE,
               remove_url = TRUE)

# Minimum frequency = 20
dfm <- dfm(toks)
dfm <- dfm_trim(dfm, min_docfreq = 20)


# date fix
docvars(dfm, "date") <- lubridate::dmy(dfm@docvars$date)
```

2.  Convert the DFM into STM format and fit an STM model with `k=35` topics.  

```{r}
###############################################################################
# I struggled to get prevalence code working. I reconfigured date
# with lubridate above but did not have time to run the alogrithim again
############################################################################

stmdfm <- convert(dfm, to = "stm")

# Set k
#K <- 35


# STM algorithm
#modelFit <- stm(documents = stmdfm$documents,
#                vocab = stmdfm$vocab,
#                K = K,
#                #prevalence = ~ s(month(date)),
#                #prevalence = ~ s(as.numeric(date_month)), 
#                data = stmdfm$meta,
#                max.em.its = 500,
#                init.type = "Spectral",
#                seed = 1234,
 #               verbose = TRUE)

# Model upload
model_fit <- readRDS("C:/Users/User/OneDrive - TCDUD.onmicrosoft.com/Documents/ASDS/QTA/modelFit")

```

3.  Interpret the topics generated by the STM model.  Discuss the prevalence and top terms of each topic.  Provide a list of the labels you have associated with each estimated topic.  For each topic, justify your labelling decision. (Hint: You will want to cite excerpts from typical tweets of a given topic.  Also, use the date variable to inform estimates of topic prevalence.).  

```{r}
## 3. Interpret Topic model 
# Inspect most probable terms in each topic
labelTopics(modelFit)

# plotting frequent terms
plot.STM(modelFit, 
         type = "summary", 
         labeltype = "frex", # plot according to FREX metric
         text.cex = 0.7,
         main = "Topic prevalence and top terms")

# Wordcloud to visualise terms
cloud(modelFit,
      topic = 1,
      scale = c(2.5, 0.3),
      max.words = 50)
```

4.  Topic model validation.  Demonstrate and interpret the semantic and predictive validity of the model.  Also discuss the quality of topics in terms of semantic coherence and top exclusivity.  Discuss how you would show construct validity.

```{r}
######################################
#Couldn't process without correct date
#####################################
```

5.  What insights can be gleaned about right-wing media coverage of the 2016 US election?  What election-related topics were derived from the model?  What interesting temporal patterns exist?  Why might the prevalence of certain important topics vary over 2016?  Provide evidence in support of your answers.

